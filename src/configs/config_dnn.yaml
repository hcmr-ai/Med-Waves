# DNN Training Configuration
# Configuration for training the U-Net based wave height correction model

# Data configuration
data:
  data_path: "/opt/dlami/nvme/preprocessed_subsampled_step_5" # "/mnt/ebs/" # "/Users/deeplab/Documents/projects/hcmr/data/hourly/" # "s3://medwav-dev-data/parquet/hourly/"
  file_pattern: "WAVEAN*.pt"
  train_year: [2021, 2022]
  val_year: [2023]
  val_months: [1, 4, 6, 7, 9, 10]
  test_year: 2023
  test_months: [2, 5, 8, 11]
  patch_size: null #[256, 256] # Random crop size for training (256x256 for proper U-Net bottleneck)
  max_files: null # Limit number of files for testing, null = use all
  random_seed: 42
  subsample_step: null
  normalize_target: false

  # Data preprocessing
  target_column: "corrected_VHM0" # Target variable
  predict_bias: true # If true, predict bias (corrected_VHM0 - VHM0), if false, predict corrected_VHM0
  excluded_columns: [
      "time",
      "latitude",
      "longitude",
      "timestamp",
      "VTM02",
      "corrected_VTM02",
      "WDIR",
      "VMDR",
    ] # Columns to exclude from input
  handle_nan: true # Handle NaN values with masking
  normalizer_path: "scalers/BU24h_zscore.pkl"

# Model configuration
model:
  in_channels: 15 # Number of input channels (all features except excluded)
  learning_rate: 1e-4
  loss_type: "mse" # Options: "mse", "weighted_mse"
  filters: [32, 64]
  dropout: 0.2

  # Learning rate scheduler configuration
  lr_scheduler:
    type: "ReduceLROnPlateau" # Options: "ReduceLROnPlateau", "CosineAnnealingLR", "StepLR", "ExponentialLR", "none"
    monitor: "val_loss" # Metric to monitor for ReduceLROnPlateau
    mode: "min" # "min" for loss, "max" for accuracy
    factor: 0.5 # Factor to reduce LR by
    patience: 5 # Epochs to wait before reducing LR
    min_lr: 1e-7 # Minimum learning rate
    # verbose: true # Print LR changes

    # Additional scheduler parameters
    T_max: 50 # For CosineAnnealingLR: max epochs
    step_size: 10 # For StepLR: epochs between LR reductions
    gamma: 0.1 # For StepLR/ExponentialLR: LR reduction factor

# Training configuration
training:
  batch_size: 64
  max_epochs: 100
  num_workers: 8
  pin_memory: true
  accelerator: "gpu" # Options: "gpu", "cpu", "auto"
  devices: 1
  precision: "32" # Full precision training (16 or 32)
  log_every_n_steps: 20
  early_stopping_patience: 10
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  fast_dev_run: false
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  benchmark: false
  prefetch_factor: 1
  val_check_interval: null
# Checkpoint configuration
checkpoint:
  resume_from_checkpoint: null # Path to checkpoint file to resume from
  checkpoint_dir: "/opt/dlami/nvme/preprocessed/checkpoints_100" # Local directory for fast access
  s3_sync_dir: "s3://medwav-dev-data/checkpoints/dnn_training_subsample_step_5_small_filters_bias_100" # S3 backup for spot instances
  save_last: true
  sync_frequency: 5 # Sync to S3 every N epochs

# Logging configuration
logging:
  log_dir: "/opt/dlami/nvme/preprocessed/logs"
  experiment_name: "dnn_wave_correction_subsample_step_5_small_filters_bias_100"
  use_comet: true
  comet_tags: ["wave_height", "bu_net", "bias_correction"]
  comet_notes: "DNN training for wave height correction using U-Net architecture"
