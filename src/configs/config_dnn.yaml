# DNN Training Configuration
# Configuration for training the U-Net based wave height correction model

# Data configuration
data:
  data_path: "/opt/dlami/nvme/preprocessed/" # "/mnt/ebs/" # "/Users/deeplab/Documents/projects/hcmr/data/hourly/" # "s3://medwav-dev-data/parquet/hourly/"
  file_pattern: "WAVEAN*_h*.pt"
  train_year: [2020, 2021]
  val_year: [2022]
  val_months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  test_year: [2023]
  test_months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  patch_size: [256, 256] # Smaller patches = faster training (was [128, 128])
  stride: [256, 256] # Stride for sliding window
  min_valid_pixels: 0.3 # Only keep patches with >30% sea pixels
  use_balanced_sampling: false # Use balanced sampling (equal samples per wave height bin)
  max_files: null # Limit number of files for testing, null = use all
  random_seed: 42
  subsample_step: null # Data already subsampled in files, no additional runtime subsampling
  use_cache: false # Cache files in memory (reduces repeated I/O)
  max_cache_size: 20 # Number of files to keep in cache per worker (20 files Ã— 44MB = 880MB per worker)

  # Data preprocessing
  # target_column: corrected_VHM0 # "corrected_VTM02" # Target variable
  target_columns: # Multi-task: {task_name: column_name}. Single-task: {task_name: column_name}
    vhm0: "corrected_VHM0"
    vtm02: "corrected_VTM02"
  predict_bias: false # If true, predict bias (corrected_VHM0 - VHM0), if false, predict corrected_VHM0
  normalize_target: true
  excluded_columns: [
      "time",
      "latitude",
      "longitude",
      "timestamp",
      # "VTM02",
      "corrected_VTM02",
      "WDIR",
      "VMDR",
    ] # Columns to exclude from input
  handle_nan: true # Handle NaN values with masking
  normalizer_path: "scalers/BU24h_zscore_target_20_21_all_with_corrected.pkl"

# Model configuration
model:
  model_type: "transunet" # Options: "nick" (simple), "geo" (geophysical), "enhanced" (nick+geo features), "transunet" (transformer-based), "swinunet" (swin transformer-based), "transunet_gan" (transformer-based GAN)
  in_channels: 15 # Number of input channels (all features except excluded)
  learning_rate: 1e-4 # 1e-3
  loss_type: "mse" # Options: "mse", "weighted_mse", "smooth_l1", "multi_bin_weighted_mse", "multi_bin_weighted_smooth_l1", "pixel_switch_mse", "mse_perceptual", "mse_ssim", "mse_ssim_perceptual", "mse_mdn", "mdn", "mse_gan"
  filters: [64, 128, 256]
  dropout: 0
  add_vhm0_residual: false
  vhm0_channel_index: 0
  weight_decay: 1e-4 #1e-4
  upsample_mode: "nearest" # Options: "nearest" (bilinear interpolation), "transpose" (convolutional transpose)
  optimizer_type: "Adam" # Options: "Adam", "AdamW", "SGD"
  use_mdn: false # Options: true, false
  lambda_adv: 0.1 # Weight for adversarial loss
  n_discriminator_updates: 3 # Number of times to update discriminator per generator update (for GAN training)
  discriminator_lr_multiplier: 2.0 # Multiplier for discriminator learning rate (e.g., 2.0 = 2x generator LR)
  # Learning rate scheduler configuration
  lr_scheduler:
    type: LambdaLR # Options: "ReduceLROnPlateau", "CosineAnnealingLR", "StepLR", "ExponentialLR", "none", "CosineAnnealingWarmupRestarts", "LambdaLR"
    monitor: "val_loss" # Metric to monitor for ReduceLROnPlateau
    mode: "min" # "min" for loss, "max" for accuracy
    factor: 0.5 # Factor to reduce LR by
    patience: 5 # Epochs to wait before reducing LR
    min_lr: 1e-7 # Minimum learning rate
    # verbose: true # Print LR changes

    # Additional scheduler parameters
    T_max: 50 # For CosineAnnealingLR: max epochs
    eta_min: 1e-6
    warmup_steps: 0.01 # For CosineAnnealingLR: warmup steps
    step_size: 10 # For StepLR: epochs between LR reductions
    gamma: 0.1 # For StepLR/ExponentialLR: LR reduction factor

  # Task configuration. Used for both single and multi-task training. Define one task if single-task training, or multiple tasks if multi-task training.
  tasks_config:
    - name: "vhm0"
      weight: 1.0
    - name: "vtm02"
      weight: 0.5
# Training configuration
training:
  batch_size: 64
  accumulate_grad_batches: 1 # Larger batches for 64x64 patches (4x smaller than 128x128)
  max_epochs: 100
  num_workers: 4
  persistent_workers: true # Not needed with num_workers=0
  pin_memory: true
  accelerator: "gpu" # Options: "gpu", "cpu", "auto"
  devices: 1
  precision: "32" # Full precision training (16 or 32)
  log_every_n_steps: 20
  early_stopping_patience: 20
  save_top_k: 25
  monitor: "val_loss"
  mode: "min"
  fast_dev_run: false
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  benchmark: false
  prefetch_factor: 2 # Increased from 1: prefetch 2 batches per worker (helps with I/O)
  val_check_interval: null
  use_swa: false
  use_ema: false
  finetune_model: false
  freeze_encoder_layers: false
  pixel_switch_threshold_m: 0.45
  gradient_clip_val: null
  gradient_clip_algorithm: "norm"
  aggressive_freeze: false
# Checkpoint configuration
checkpoint:
  resume_from_checkpoint: null # -21_mse_64_lambda_lr/epoch=38-val_loss=0.01.ckpt # Localepoch=01-val_loss=0.04.ckpt" # Path to checkpoint file to resume from
  checkpoint_dir: "/opt/dlami/nvme/checkpoints_full_20-21_mse_64_lambda_lr_256" # Local directory for fast access
  s3_sync_dir: "s3://medwav-dev-data/checkpoints/dnn_training_full_20-21_mse_64_lambda_lr_256" # S3 backup for spot instances
  save_last: true
  sync_frequency: 2 # Sync to S3 every N epochs

# Logging configuration
logging:
  log_dir: "/opt/dlami/nvme/logs_full_20-21_mse_64_lambda_lr_256"
  experiment_name: "dnn_training_full_20-21_mse_64_lambda_lr_256"
  use_comet: true
  comet_tags:
    [
      "wave_height",
      "transunet",
      "mse",
      "bias_correction",
      "lambda_lr",
      "full_dataset",
      "256_patch_size",
    ]
  comet_notes: "DNN training for wave height correction using Transformer-based U-Net architecture with LambdaLR scheduler"
