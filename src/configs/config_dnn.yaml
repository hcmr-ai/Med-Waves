# DNN Training Configuration
# Configuration for training the U-Net based wave height correction model

# Data configuration
data:
  data_path: "/opt/dlami/nvme/preprocessed_subsampled_step_5" # "/mnt/ebs/" # "/Users/deeplab/Documents/projects/hcmr/data/hourly/" # "s3://medwav-dev-data/parquet/hourly/"
  file_pattern: "WAVEAN*.pt"
  train_year: [2017, 2018, 2019, 2020, 2021]
  val_year: [2022]
  val_months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  test_year: [2023]
  test_months: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  patch_size: null #[64, 64] #[64, 64] #[128, 128] # Reduced to fit PRE-subsampled data (76x261 stored in files)
  min_valid_pixels: 0.3 # Only keep patches with >30% sea pixels
  use_balanced_sampling: false # Use balanced sampling (equal samples per wave height bin)
  max_files: null # Limit number of files for testing, null = use all
  random_seed: 42
  subsample_step: null # Data already subsampled in files, no additional runtime subsampling
  use_cache: false

  # Data preprocessing
  target_column: "corrected_VHM0" # Target variable
  predict_bias: false # If true, predict bias (corrected_VHM0 - VHM0), if false, predict corrected_VHM0
  normalize_target: true
  excluded_columns: [
      "time",
      "latitude",
      "longitude",
      "timestamp",
      # "VTM02",
      "corrected_VTM02",
      "WDIR",
      "VMDR",
    ] # Columns to exclude from input
  handle_nan: true # Handle NaN values with masking
  normalizer_path: "scalers/BU24h_zscore_target_21_22_with_corrected.pkl"

# Model configuration
model:
  model_type: "transunet" # Options: "nick" (simple), "geo" (geophysical), "enhanced" (nick+geo features), "transunet" (transformer-based), "swinunet" (swin transformer-based)
  in_channels: 15 # Number of input channels (all features except excluded)
  learning_rate: 1e-4 # 1e-3
  loss_type: "mse_mdn" # Options: "mse", "weighted_mse", "smooth_l1", "multi_bin_weighted_mse", "multi_bin_weighted_smooth_l1", "pixel_switch_mse", "mse_perceptual", "mse_ssim", "mse_ssim_perceptual", "mse_mdn", "mdn"
  filters: [64, 128, 256]
  dropout: 0
  add_vhm0_residual: false
  vhm0_channel_index: 0
  weight_decay: 1e-4 #1e-4
  upsample_mode: "nearest" # Options: "nearest" (bilinear interpolation), "transpose" (convolutional transpose)
  optimizer_type: "Adam" # Options: "Adam", "AdamW", "SGD"
  use_mdn: true # Options: true, false
  # Learning rate scheduler configuration
  lr_scheduler:
    type: LambdaLR # Options: "ReduceLROnPlateau", "CosineAnnealingLR", "StepLR", "ExponentialLR", "none", "CosineAnnealingWarmupRestarts", "LambdaLR"
    monitor: "val_loss" # Metric to monitor for ReduceLROnPlateau
    mode: "min" # "min" for loss, "max" for accuracy
    factor: 0.5 # Factor to reduce LR by
    patience: 5 # Epochs to wait before reducing LR
    min_lr: 1e-7 # Minimum learning rate
    # verbose: true # Print LR changes

    # Additional scheduler parameters
    T_max: 50 # For CosineAnnealingLR: max epochs
    eta_min: 1e-6
    warmup_steps: 0.1 # For CosineAnnealingLR: warmup steps
    step_size: 10 # For StepLR: epochs between LR reductions
    gamma: 0.1 # For StepLR/ExponentialLR: LR reduction factor

# Training configuration
training:
  batch_size: 64
  max_epochs: 100
  num_workers: 8
  pin_memory: true
  accelerator: "gpu" # Options: "gpu", "cpu", "auto"
  devices: 1
  precision: "32" # Full precision training (16 or 32)
  log_every_n_steps: 20
  early_stopping_patience: 20
  save_top_k: 25
  monitor: "val_loss"
  mode: "min"
  fast_dev_run: false
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  benchmark: false
  prefetch_factor: 1
  val_check_interval: null
  use_swa: false
  use_ema: false
  finetune_model: false
  freeze_encoder_layers: false
  pixel_switch_threshold_m: 0.45
  gradient_clip_val: null
  gradient_clip_algorithm: "norm"
  aggressive_freeze: false
# Checkpoint configuration
checkpoint:
  resume_from_checkpoint: null # Localepoch=01-val_loss=0.04.ckpt" # Path to checkpoint file to resume from
  checkpoint_dir: "/opt/dlami/nvme/checkpoints_100_val_22_test_23_transunet_17-21_mse_mdn_64_lambda_lr" # Local directory for fast access
  s3_sync_dir: "s3://medwav-dev-data/checkpoints/dnn_training_subsample_step_5_100_val_22_test_23_transunet_17-21_mse_mdn_64_lambda_lr" # S3 backup for spot instances
  save_last: true
  sync_frequency: 2 # Sync to S3 every N epochs

# Logging configuration
logging:
  log_dir: "/opt/dlami/nvme/logs"
  experiment_name: "dnn_wave_correction_subsample_step_5_100_val_22_test_23_transunet_17-21_mse_mdn_64_lambda_lr"
  use_comet: true
  comet_tags: ["wave_height", "transunet", "bias_correction", "lambda_lr", "mse_mdn"]
  comet_notes: "DNN training for wave height correction using Transformer-based U-Net architecture with LambdaLR scheduler"
