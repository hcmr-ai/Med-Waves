# Full Dataset Trainer Configuration
# This configuration is optimized for loading all data into memory
# and training models on the complete dataset for research purposes

# Model configuration
model:
  type: "xgb" # Options: xgb, rf, elasticnet, lasso, ridge, eqm, delta
  n_estimators: 2000
  max_depth: 6
  learning_rate: 0.08
  subsample: 0.8
  colsample_bytree: 0.8
  reg_alpha: 0.01
  reg_lambda: 1.0
  min_child_weight: 2
  random_state: 42
  n_jobs: -1
  early_stopping_rounds: 50
  eval_metric: ["rmse", "mae"]
  objective: "reg:squarederror"
  tree_method: hist
  gamma: 0.5
  max_bin: 256
  verbose: false

  # EQM-specific configuration (only used when type: "eqm")
  eqm:
    quantile_resolution: 1000
    use_kde: true
    extrapolation_method: "constant" # Options: constant, linear, nearest
    kde_bandwidth: null # Auto if null
    variables: ["VHM0"] # Variables to correct

  # Delta-specific configuration (only used when type: "delta")
  delta:
    variables: ["VHM0"] # Variables to correct
    method: "mean_bias" # Options: mean_bias, median_bias

# Data configuration
data:
  # Data splitting strategy
  split:
    type: "year_based" # Options: random, temporal, stratified, year_based
    # For year_based split:
    train_end_year: 2022 # Train data up to this year (exclusive)
    test_start_year: 2023 # Test data from this year onwards
    val_months: [12] # Months to use for validation (August 2021)
    eval_months: [12] # Months to use for evaluation (December 2023) - only these months will be loaded from S3
    # For other split types:
    test_size: 0.2 # 20% for testing
    val_size: 0.2 # 20% for validation
    random_state: 42
    n_bins: 10 # For stratified splitting

  # Data loading
  data_path: "s3://medwav-dev-data/parquet/hourly/" # Can be local path or S3 URI (s3://bucket/prefix)
  file_pattern: "*.parquet"
  target_column: "vhm0_y"

  # S3 configuration (only needed if using S3)
  s3:
    aws_profile: null # AWS profile name (optional)
    region: "eu-central-1" # AWS region (optional)
    max_retries: 10 # Max retries for S3 operations

  # Parallel loading configuration
  parallel_loading: true # Enable parallel file loading for faster data loading
  max_workers: 8

# Feature engineering configuration
feature_block:
  predict_bias: true # If true, the model will predict the bias instead of the vhm0_y
  features_to_exclude:
    - "corrected_VTM02"
    - "time"
    - "lat"
    - "lon"
    - "VMDR"
    - "WDIR"

  # Sampling (optional - can be disabled for full dataset)
  max_samples_per_file: 500000 # Set to null to use all data
  sampling_strategy: "robust_stratified" # Options: none, random, per_location, temporal, robust_stratified
  # NOTE: robust_stratified is designed for multi-region balance and will be automatically
  # switched to single_region_robust when regional_training is enabled
  samples_per_location: 6
  samples_per_hour: 100
  sampling_seed: 42

  # Preprocessing
  scaler: "standard" # Options: standard, robust, minmax, null (no scaling)

  # Regional scaling (applies different scaling to different geographic regions)
  # NOTE: Requires a valid scaler (standard, robust, minmax) - cannot be used with scaler: "null"
  regional_scaling:
    enabled: false # Enable regional scaling for Atlantic vs Mediterranean

  # Geographic context features (adds geographic features to the model)
  use_geo_context:
    enabled: false # Enable geographic context features (distances, fetch, bathymetry)
    include_basin: false # Include basin categorical feature (0=Atlantic, 1=Mediterranean, 2=Eastern Med)

  # Lag features (uses previous time steps of input variables)
  lag_features:
    enabled: false # Enable lag features for temporal patterns
    lags:
      U10: [6, 12, 24] # Previous degraded wave heights (hours ago)
      V10: [6, 12, 24]
  # Regional weighting (gives different importance to different regions during training)
  regional_weighting:
    enabled: false # Enable regional weighting for Atlantic vs Mediterranean
    weights:
      0: 3.0 # atlantic - 50% more weight for Atlantic data
      1: 1.0 # mediterranean - Normal weight for Mediterranean data
      2: 1.0 # eastern_med - Normal weight for Eastern Mediterranean data

  # Regional training (train only in specific regions)
  # NOTE: When enabled, robust_stratified sampling will be automatically switched to single_region_robust
  # which maintains wave height stratification and temporal coverage within the selected region(s)
  regional_training:
    enabled: false # Enable regional training (train only in specified regions)
    training_regions: [0] # Options: [0] (atlantic), [1] (mediterranean), [2] (eastern_med), or combinations
    # Alternative: use coordinate bounds instead of region names
    # training_bounds:
    #   lon_min: -10.0
    #   lon_max: -5.0
    #   lat_min: 35.0
    #   lat_max: 45.0

  # Feature selection
  feature_selection:
    enabled: false
    type: "kbest" # Options: kbest, rfe
    k: 100

  # Dimension reduction
  dimension_reduction:
    enabled: false
    type: "pca" # Options: pca, svd
    n_components: 50

  # Sea-bin performance metrics (analyze performance across different wave height ranges)
  sea_bin_metrics:
    enabled: true # Set to true to enable sea-bin metrics
    bins:
      - name: "calm"
        min: 0.0
        max: 1.0
        description: "Calm conditions"
      - name: "light"
        min: 1.0
        max: 2.0
        description: "Light seas"
      - name: "moderate"
        min: 2.0
        max: 3.0
        description: "Moderate seas"
      - name: "rough"
        min: 3.0
        max: 4.0
        description: "Rough seas"
      - name: "very_rough"
        min: 4.0
        max: 6.0
        description: "Very rough seas"
      - name: "high"
        min: 6.0
        max: 9.0
        description: "High seas"
      - name: "very_high"
        min: 9.0
        max: 12.0
        description: "Very high seas"
      - name: "phenomenal"
        min: 12.0
        max: 999.0
        description: "Phenomenal seas"

# Evaluation configuration
evaluation:
  metrics: ["rmse", "mae", "bias", "pearson"]

# Training diagnostics
training_diagnostics:
  enabled: true
  plots_save_path: "diagnostic_plots"
  save_training_history: true
  create_learning_curves: true
  create_residual_plots: true
  create_prediction_plots: true
  create_regional_analysis: true # Enable regional performance analysis plots
  create_sea_bin_analysis: true # Enable sea-bin performance analysis plots
  create_spatial_maps: true # Enable aggregated spatial maps
  create_regional_error_box_plots: true
  create_wave_height_performance_plots: true

# Logging configuration
logging:
  level: "INFO"
  save_logs: true
  log_file: "full_dataset_training.log"
  use_comet: true
  comet_project: hcmr-ai
  comet_workspace: ioannisgkinis

# Memory monitoring configuration
memory_monitoring:
  enabled: true
  log_detailed_memory: true
  log_system_memory: true
  log_to_comet: true

# Output configuration
output:
  model_save_path: "trained_models"
  experiment_name: "xgb_500K_samples_robust_stratified"

  # S3 configuration for saving results
  s3:
    enabled: true
    bucket: "medwav-dev-data"
    prefix: "experiments/full_dataset_experiment/" # the code will add experiment_name + timestamp at the end
    region: "eu-central-1"
    aws_profile: null # Use default profile if null
