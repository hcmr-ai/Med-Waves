# Configuration for model evaluation with spatial analysis

# Data configuration
data:
  # Data path will be specified via command line
  target_column: "vhm0_y" # Target column name (currently hardcoded in code)
  model_path: "s3://medwav-dev-data/experiments/full_dataset_experiment/xgb_500k_samples_20250917/models/"
  data_path: "s3://medwav-dev-data/parquet/hourly/"
  file_pattern: "*.parquet"

# Feature configuration (synchronized with training pipeline)
feature_block:
  # Features to exclude from the feature set
  features_to_exclude:
    - "corrected_VTM02"
    - "time"
    - "lat"
    - "lon"
    - "latitude"
    - "longitude"

  # Geographic context features
  use_geo_context:
    enabled: false # Set to true to enable geographic context features
    include_basin: true # Include basin categorical feature (0=Atlantic, 1=Mediterranean, 2=Eastern Med)

  # Lag features for temporal patterns
  lag_features:
    enabled: false # Set to true to enable lag features
    lags:
      vhm0_x: [1, 3, 6] # Lag hours for significant wave height
      wspd: [1, 3, 6] # Lag hours for wind speed
      vtm02: [1, 3, 6] # Lag hours for wave period
      wdir: [1, 3, 6] # Lag hours for wind direction
      vmdr: [1, 3, 6] # Lag hours for wave direction

  # Regional training (filter to specific regions)
  regional_training:
    enabled: false # Set to true to enable regional training
    training_regions: ["atlantic"] # Regions to include in training

  # Sea-bin metrics for performance analysis across wave height ranges
  sea_bin_metrics:
    enabled: true # Set to true to enable sea-bin metrics
    bins:
      - name: "calm"
        min: 0.0
        max: 0.5
        description: "Calm conditions"
      - name: "light"
        min: 0.5
        max: 1.0
        description: "Light seas"
      - name: "moderate"
        min: 1.0
        max: 2.0
        description: "Moderate seas"
      - name: "rough"
        min: 2.0
        max: 4.0
        description: "Rough seas"
      - name: "very_rough"
        min: 4.0
        max: 6.0
        description: "Very rough seas"
      - name: "high"
        min: 6.0
        max: 10.0
        description: "High seas"

# Preprocessing configuration (synchronized with training pipeline)
preprocessing:
  scaler: "standard" # Options: "standard", "robust", "minmax", "null"

  # Regional scaling (applies different scalers to different regions)
  regional_scaling:
    enabled: false # Set to true to enable regional scaling
    # NOTE: Cannot be used with scaler: "null"

# Evaluation configuration
evaluation:
  year: "2023"
  max_files: null # Process all December 2023 files (31 days) for faster evaluation
  # Spatial metrics to compute and plot
  spatial_metrics:
    - "bias"
    - "mae"
    - "rmse"
    - "pearson"
    - "diff"
    - "var_true"
    - "var_pred"
    - "snr"
    - "snr_db"

  # Temporal analysis
  temporal_analysis:
    enabled: true # Let users skip temporal analysis
    daily_metrics: true
    monthly_metrics: true
    seasonal_metrics: true
    create_plots: true

  # Plot settings (currently hardcoded in code)
  plots:
    marker_size: 8
    alpha: 0.85
    colormap: "RdYlBu_r"
    metrics_to_plot: ["bias", "mae", "rmse", "diff"]

# S3 configuration for model loading and results saving
s3:
  enabled: true # Enable S3 model loading
  bucket: "medwav-dev-data" # S3 bucket name
  region: "eu-central-1" # AWS region
  aws_profile: null # AWS profile name (optional, uses default if null)

# Output configuration
output:
  output_dir: "evaluation_results"
  save_plots: true # Let users skip plot generation

  # S3 configuration for saving results
  s3:
    enabled: false # Set to true to enable S3 saving
    bucket: "medwav-dev-data" # Replace with your S3 bucket
    prefix: "experiments/full_dataset_experiment/xgb_500k_samples_20250917/evaluation_results" # S3 prefix for organizing results
    region: "eu-central-1" # AWS region
    aws_profile: null # AWS profile name (optional, uses default if null)

# Parallel processing configuration
parallel_processing:
  enabled: true # Re-enable parallel processing for speed
  n_workers: 4 # Use 4 workers for processing all 365 files
  batch_size: 8 # Process 10 files per batch for efficiency
  use_multiprocessing: false # Use ProcessPoolExecutor for true parallelism
  max_workers: 6 # Maximum number of workers
