# Configuration for model evaluation with spatial analysis

# Data configuration
data:
  # Data path will be specified via command line
  target_column: "vhm0_y" # Target column name (currently hardcoded in code)
  model_path: "trained_models"
  data_path: "/Users/deeplab/Documents/projects/hcmr/data/hourly"
  file_pattern: "*.parquet"

# Evaluation configuration
evaluation:
  year: "2023"
  max_files: 3
  # Spatial metrics to compute and plot
  spatial_metrics:
    - "bias"
    - "mae"
    - "rmse"
    - "pearson"
    - "diff"
    - "var_true"
    - "var_pred"
    - "snr"
    - "snr_db"

  # Temporal analysis
  temporal_analysis:
    enabled: true # Let users skip temporal analysis
    daily_metrics: true
    monthly_metrics: true
    seasonal_metrics: true
    create_plots: true

  # Plot settings (currently hardcoded in code)
  plots:
    marker_size: 8
    alpha: 0.85
    colormap: "RdYlBu_r"
    metrics_to_plot: ["bias", "mae", "rmse", "diff"]

# Output configuration
output:
  output_dir: "evaluation_results"
  save_plots: true # Let users skip plot generation

  # S3 configuration for saving results
  s3:
    enabled: false # Set to true to enable S3 saving
    bucket: "your-s3-bucket-name" # Replace with your S3 bucket
    prefix: "evaluation_results" # S3 prefix for organizing results
    region: "us-east-1" # AWS region
    aws_profile: null # AWS profile name (optional, uses default if null)

# Parallel processing configuration
parallel_processing:
  enabled: true # Let users control parallel processing
  n_workers: 2 # Number of parallel workers (default: min(cpu_count(), 8))
  batch_size: 10 # Number of files to process in each batch
  use_multiprocessing: true # Use ProcessPoolExecutor (true) or ThreadPoolExecutor (false)
  max_workers: 8 # Maximum number of workers (default: n_workers)
