{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObzoaPfCv56S"
   },
   "source": [
    "#Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEDunrszv4td"
   },
   "outputs": [],
   "source": [
    "!pip install xarray netCDF4 dask\n",
    "\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from google.colab import drive\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Cropping2D,\n",
    "    Input,\n",
    "    Lambda,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgwW8z-lwIWG"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "file_path_X = \"/content/drive/MyDrive/AI_project/without_reduced/5step/\"\n",
    "file_path_Y = \"/content/drive/MyDrive/AI_project/with_reduced/5step/\"\n",
    "\n",
    "output_directory = \"/content/drive/MyDrive/dataset/output\"\n",
    "\n",
    "files_X = sorted(glob.glob(file_path_X + \"WAVEAN20*.nc\"))\n",
    "files_Y = sorted(glob.glob(file_path_Y + \"WAVEAN20*.nc\"))\n",
    "\n",
    "ds_features = xr.open_mfdataset(files_X, chunks=\"auto\", engine=\"netcdf4\")\n",
    "ds_labels = xr.open_mfdataset(files_Y, chunks=\"auto\", engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPcdO7fLwU6f"
   },
   "source": [
    "Features:\n",
    "\n",
    "    VHM0_uncorrected\n",
    "    WSPD\n",
    "    WDIR_cos\n",
    "    WDIR_sin\n",
    "    VMDR_cos\n",
    "    VMDR_sin\n",
    "    month_cos\n",
    "    month_sin\n",
    "    hour_cos\n",
    "    hour_sin\n",
    "\n",
    "Labels:\n",
    "\n",
    "    VHM0_groundtruth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AvaeBUiwcqZ"
   },
   "outputs": [],
   "source": [
    "print(ds_features.data_vars)\n",
    "print(ds_labels.data_vars)\n",
    "\n",
    "ds_features[\"WDIR_cos\"] = np.cos(np.deg2rad(ds_features[\"WDIR\"]))\n",
    "ds_features[\"WDIR_sin\"] = np.sin(np.deg2rad(ds_features[\"WDIR\"]))\n",
    "ds_features[\"VMDR_cos\"] = np.cos(np.deg2rad(ds_features[\"VMDR\"]))\n",
    "ds_features[\"VMDR_sin\"] = np.sin(np.deg2rad(ds_features[\"VMDR\"]))\n",
    "\n",
    "\n",
    "ds_features[\"month_cos\"] = np.cos(2 * np.pi * (ds_features.time.dt.month - 1) / 12)\n",
    "ds_features[\"month_sin\"] = np.sin(2 * np.pi * (ds_features.time.dt.month - 1) / 12)\n",
    "ds_features[\"hour_cos\"] = np.cos(2 * np.pi * ds_features.time.dt.hour / 24)\n",
    "ds_features[\"hour_sin\"] = np.sin(2 * np.pi * ds_features.time.dt.hour / 24)\n",
    "\n",
    "\n",
    "ds_features[\"month_cos\"] = ds_features[\"month_cos\"].broadcast_like(ds_features[\"VHM0\"])\n",
    "ds_features[\"month_sin\"] = ds_features[\"month_sin\"].broadcast_like(ds_features[\"VHM0\"])\n",
    "ds_features[\"hour_cos\"] = ds_features[\"hour_cos\"].broadcast_like(ds_features[\"VHM0\"])\n",
    "ds_features[\"hour_sin\"] = ds_features[\"hour_sin\"].broadcast_like(ds_features[\"VHM0\"])\n",
    "\n",
    "\n",
    "labels = ds_labels[\"VHM0\"]  # shape: (time, lat, lon)\n",
    "features = xr.Dataset(\n",
    "    {\n",
    "        \"VHM0_uncorr\": ds_features[\"VHM0\"],\n",
    "        \"WSPD\": ds_features[\"WSPD\"],\n",
    "        \"WDIR_cos\": ds_features[\"WDIR_cos\"],\n",
    "        \"WDIR_sin\": ds_features[\"WDIR_sin\"],\n",
    "        \"VMDR_cos\": ds_features[\"VMDR_cos\"],\n",
    "        \"VMDR_sin\": ds_features[\"VMDR_sin\"],\n",
    "        \"month_cos\": ds_features[\"month_cos\"],\n",
    "        \"month_sin\": ds_features[\"month_sin\"],\n",
    "        \"hour_cos\": ds_features[\"hour_cos\"],\n",
    "        \"hour_sin\": ds_features[\"hour_sin\"],\n",
    "    }\n",
    ").to_array(dim=\"channel\")  # shape: (channel, time, latitude, longitude)\n",
    "\n",
    "# Transpose to (time, latitude, longitude, channel)\n",
    "features = features.transpose(\"time\", \"latitude\", \"longitude\", \"channel\")\n",
    "channels_number = features.sizes[\"channel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ui49ocSsw39m"
   },
   "source": [
    "# Split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6yBj_ktw5NA"
   },
   "outputs": [],
   "source": [
    "train_features = features.sel(time=slice(\"2021-01-01\", \"2022-12-31\"))\n",
    "train_labels = labels.sel(time=slice(\"2021-01-01\", \"2022-12-31\"))\n",
    "\n",
    "val_features = xr.concat(\n",
    "    [\n",
    "        features.sel(time=slice(\"2023-01-01\", \"2023-01-31\")),  # Winter\n",
    "        features.sel(time=slice(\"2023-04-01\", \"2023-04-30\")),  # Spring\n",
    "        features.sel(time=slice(\"2023-07-01\", \"2023-07-31\")),  # Summer\n",
    "        features.sel(time=slice(\"2023-10-01\", \"2023-10-31\")),  # Fall\n",
    "    ],\n",
    "    dim=\"time\",\n",
    ")\n",
    "\n",
    "val_labels = xr.concat(\n",
    "    [\n",
    "        labels.sel(time=slice(\"2023-01-01\", \"2023-01-31\")),\n",
    "        labels.sel(time=slice(\"2023-04-01\", \"2023-04-30\")),\n",
    "        labels.sel(time=slice(\"2023-07-01\", \"2023-07-31\")),\n",
    "        labels.sel(time=slice(\"2023-10-01\", \"2023-10-31\")),\n",
    "    ],\n",
    "    dim=\"time\",\n",
    ")\n",
    "\n",
    "test_features = xr.concat(\n",
    "    [\n",
    "        features.sel(time=slice(\"2023-02-01\", \"2023-02-28\")),  # Winter\n",
    "        features.sel(time=slice(\"2023-05-01\", \"2023-05-31\")),  # Spring\n",
    "        features.sel(time=slice(\"2023-08-01\", \"2023-08-31\")),  # Summer\n",
    "        features.sel(time=slice(\"2023-11-01\", \"2023-11-30\")),  # Fall\n",
    "    ],\n",
    "    dim=\"time\",\n",
    ")\n",
    "\n",
    "test_labels = xr.concat(\n",
    "    [\n",
    "        labels.sel(time=slice(\"2023-02-01\", \"2023-02-28\")),\n",
    "        labels.sel(time=slice(\"2023-05-01\", \"2023-05-31\")),\n",
    "        labels.sel(time=slice(\"2023-08-01\", \"2023-08-31\")),\n",
    "        labels.sel(time=slice(\"2023-11-01\", \"2023-11-30\")),\n",
    "    ],\n",
    "    dim=\"time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHu8xmxjw_sE"
   },
   "source": [
    "#Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytXjcuRBxBvU"
   },
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    \"VHM0\",\n",
    "    \"WSPD\",\n",
    "    \"WDIR_cos\",\n",
    "    \"WDIR_sin\",\n",
    "    \"VMDR_cos\",\n",
    "    \"VMDR_sin\",\n",
    "    \"month_cos\",\n",
    "    \"month_sin\",\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "]\n",
    "scalers = {}\n",
    "\n",
    "for i, name in enumerate(channel_names):\n",
    "    # Compute mean and std lazily with Dask for channel i\n",
    "    channel_data = train_features[..., i]\n",
    "    mean_val = channel_data.mean().compute()\n",
    "    std_val = channel_data.std().compute()\n",
    "    print(f\" Train set: Channel {name} - Mean: {mean_val}, Std: {std_val}\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = np.array([mean_val])\n",
    "    scaler.scale_ = np.array([std_val])\n",
    "    scalers[name] = scaler\n",
    "\n",
    "label_scaler = StandardScaler()\n",
    "train_labels_np = train_labels.compute().data.reshape(-1, 1)\n",
    "label_scaler.fit(train_labels_np)\n",
    "\n",
    "\n",
    "def scale_feature(data, scaler):\n",
    "    original_shape = data.shape\n",
    "    data_flat = data.reshape(-1, 1)\n",
    "    data_scaled = scaler.transform(data_flat)\n",
    "    return data_scaled.reshape(original_shape)\n",
    "\n",
    "\n",
    "def scale_label(data, scaler):\n",
    "    original_shape = data.shape\n",
    "    data_flat = data.reshape(-1, 1)\n",
    "    data_scaled = scaler.transform(data_flat)\n",
    "    return data_scaled.reshape(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z952vzNqxVl0"
   },
   "outputs": [],
   "source": [
    "def data_generator(features, labels, feature_scalers, label_scaler, batch_size):\n",
    "    num_samples = features.sizes[\"time\"]\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        # Load the batch into memory\n",
    "        batch_features = (\n",
    "            features.isel(time=slice(start, end)).compute().data\n",
    "        )  # shape: (batch, lat, lon, channels)\n",
    "        batch_labels = (\n",
    "            labels.isel(time=slice(start, end)).compute().data[..., None]\n",
    "        )  # shape: (batch, lat, lon, 1)\n",
    "\n",
    "        # Scaling\n",
    "        for i, name in enumerate(channel_names):\n",
    "            batch_features[..., i] = scale_feature(\n",
    "                batch_features[..., i], feature_scalers[name]\n",
    "            )\n",
    "        batch_labels = scale_label(batch_labels, label_scaler)\n",
    "\n",
    "        batch_features = np.nan_to_num(batch_features, nan=0.0)\n",
    "        batch_labels = np.nan_to_num(batch_labels, nan=0.0)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        yield batch_features.astype(np.float32), batch_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CG80ws1xZ0V"
   },
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    features_subset, labels_subset, feature_scalers, label_scaler, batch_size\n",
    "):\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(\n",
    "            features_subset, labels_subset, feature_scalers, label_scaler, batch_size\n",
    "        ),\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            (\n",
    "                None,\n",
    "                features_subset.sizes[\"latitude\"],\n",
    "                features_subset.sizes[\"longitude\"],\n",
    "                features_subset.sizes[\"channel\"],\n",
    "            ),\n",
    "            (\n",
    "                None,\n",
    "                labels_subset.sizes[\"latitude\"],\n",
    "                labels_subset.sizes[\"longitude\"],\n",
    "                1,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    train_features, train_labels, scalers, label_scaler, BATCH_SIZE\n",
    ")\n",
    "val_dataset = create_dataset(\n",
    "    val_features, val_labels, scalers, label_scaler, BATCH_SIZE\n",
    ")\n",
    "test_dataset = create_dataset(\n",
    "    test_features, test_labels, scalers, label_scaler, BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(buffer_size=100, reshuffle_each_iteration=True)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st_Ei90qyWJk"
   },
   "source": [
    "#Defining the Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrNPvC2Iybsr"
   },
   "source": [
    "**Masking in the Loss Function**\n",
    "Create a custom loss that ignores the NaN (land) locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP5hxLfwyfJt"
   },
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred, epsilon=1e-6):\n",
    "    # Binary mask: 1 for valid values, 0 for land\n",
    "    mask = tf.cast(tf.logical_not(tf.math.is_nan(y_true)), tf.float32)\n",
    "    mask_sum = tf.reduce_sum(mask)\n",
    "\n",
    "    y_true_clean = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)\n",
    "    y_pred_clean = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_pred), y_pred)\n",
    "\n",
    "    # Compute the squared error and apply the mask.\n",
    "    se = tf.square(y_true_clean - y_pred_clean) * mask\n",
    "\n",
    "    # If no valid pixels in batch, return 0.\n",
    "    mse = tf.cond(\n",
    "        tf.equal(mask_sum, 0.0),\n",
    "        lambda: 0.0,\n",
    "        lambda: tf.reduce_sum(se) / (mask_sum + epsilon),\n",
    "    )\n",
    "    return mse\n",
    "\n",
    "\n",
    "def masked_mae(y_true, y_pred, epsilon=1e-6):\n",
    "    # Mask: 1 for valid pixels, 0 for NaNs.\n",
    "    mask = tf.cast(tf.logical_not(tf.math.is_nan(y_true)), tf.float32)\n",
    "    mask_sum = tf.reduce_sum(mask)\n",
    "\n",
    "    # Replace NaNs with zeros for the error computation.\n",
    "    y_true_clean = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_true), y_true)\n",
    "    y_pred_clean = tf.where(tf.math.is_nan(y_true), tf.zeros_like(y_pred), y_pred)\n",
    "\n",
    "    # Compute the absolute error and apply the mask.\n",
    "    abs_err = tf.abs(y_true_clean - y_pred_clean) * mask\n",
    "\n",
    "    # Return 0 if no valid pixels, else the mean error.\n",
    "    mae = tf.cond(\n",
    "        tf.equal(mask_sum, 0.0),\n",
    "        lambda: 0.0,\n",
    "        lambda: tf.reduce_sum(abs_err) / (mask_sum + epsilon),\n",
    "    )\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqOXy-Yi0ldC"
   },
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "\n",
    "    def call(self, x):\n",
    "        if isinstance(self.padding[0], (list, tuple)):\n",
    "            pad_top, pad_bottom = self.padding[0]\n",
    "            pad_left, pad_right = self.padding[1]\n",
    "        else:\n",
    "            pad_top = pad_bottom = self.padding[0]\n",
    "            pad_left = pad_right = self.padding[1]\n",
    "        return tf.pad(\n",
    "            x,\n",
    "            [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]],\n",
    "            mode=\"REFLECT\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMMuYkoJza4n"
   },
   "outputs": [],
   "source": [
    "def unet_with_residual(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    padded_inputs = ReflectionPadding2D(padding=((2, 2), (1, 1)))(inputs)\n",
    "    # Encoder\n",
    "    c1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(padded_inputs)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(p1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(p2)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u2 = UpSampling2D((2, 2))(c3)\n",
    "    u2 = concatenate([u2, c2])\n",
    "    c4 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(u2)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "\n",
    "    u1 = UpSampling2D((2, 2))(c4)\n",
    "    u1 = concatenate([u1, c1])\n",
    "    c5 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(u1)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "\n",
    "    # Output correction (same spatial shape, 1 channel)\n",
    "    correction = Conv2D(1, (3, 3), padding=\"same\", activation=\"linear\")(c5)\n",
    "\n",
    "    # Residual connection\n",
    "    VHM0_raw = Lambda(lambda x: tf.expand_dims(x[..., 0], axis=-1))(padded_inputs)\n",
    "    output_padded = Add()([VHM0_raw, correction])\n",
    "    outputs = Cropping2D(cropping=((2, 2), (1, 1)))(\n",
    "        output_padded\n",
    "    )  # Removes extra padding\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCj86r6X0SYa"
   },
   "outputs": [],
   "source": [
    "sample_batch_features, sample_batch_labels = next(\n",
    "    data_generator(features, labels, scalers, label_scaler, batch_size=1)\n",
    ")\n",
    "print(\"Feature batch shape:\", sample_batch_features.shape)\n",
    "print(\"Label batch shape:\", sample_batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgzcdIIF0Xbj"
   },
   "outputs": [],
   "source": [
    "input_shape = (\n",
    "    sample_batch_features.shape[1],\n",
    "    sample_batch_features.shape[2],\n",
    "    sample_batch_features.shape[3],\n",
    ")\n",
    "print(input_shape)\n",
    "model = unet_with_residual(input_shape)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=masked_mse, metrics=[masked_mae])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igPzXFSj0wg-"
   },
   "outputs": [],
   "source": [
    "train_time_steps = train_features.sizes[\"time\"]\n",
    "\n",
    "train_batches = train_time_steps // BATCH_SIZE\n",
    "val_batches = val_features.sizes[\"time\"] // BATCH_SIZE\n",
    "test_batches = int(np.ceil(test_features.sizes[\"time\"] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGVD-Mc60yg1"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-6\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=train_batches,\n",
    "    validation_steps=val_batches,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_n0Cmez1btt"
   },
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/saved_model_BUnet_02.keras\")\n",
    "model.save_weights(\"/content/drive/MyDrive/weights_bunet02.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cono_zvI1iz"
   },
   "source": [
    " Polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Cy9hUd6I0Xj"
   },
   "outputs": [],
   "source": [
    "class PolynomialExpansion(Layer):\n",
    "    def __init__(self, degree=2, **kwargs):\n",
    "        super(PolynomialExpansion, self).__init__(**kwargs)\n",
    "        self.degree = degree\n",
    "        if self.degree != 2:\n",
    "            raise NotImplementedError(\n",
    "                \"This implementation currently supports only degree=2.\"\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, height, width, channels)\n",
    "        # For each pixel, we want to compute the original features and all quadratic terms.\n",
    "        batch_size, H, W, C = tf.unstack(tf.shape(inputs))\n",
    "        # Reshape spatial dimensions into one: (batch * H * W, C)\n",
    "        flat_inputs = tf.reshape(inputs, [-1, C])\n",
    "        # Compute the full outer product for each pixel: shape (batch*H*W, C, C)\n",
    "        outer = tf.einsum(\"bi,bj->bij\", flat_inputs, flat_inputs)\n",
    "        # Flatten the quadratic terms: shape (batch*H*W, C*C)\n",
    "        outer_flat = tf.reshape(outer, [-1, C * C])\n",
    "        # Concatenate original features and quadratic features: shape (batch*H*W, C + C*C)\n",
    "        expanded_flat = tf.concat([flat_inputs, outer_flat], axis=-1)\n",
    "        # Reshape back to (batch, H, W, new_channels)\n",
    "        new_channels = C + C * C\n",
    "        expanded = tf.reshape(expanded_flat, [batch_size, H, W, new_channels])\n",
    "        return expanded\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (\n",
    "            input_shape[0],\n",
    "            input_shape[1],\n",
    "            input_shape[2],\n",
    "            input_shape[3] + input_shape[3] * input_shape[3],\n",
    "        )\n",
    "\n",
    "\n",
    "def polynomial_model(input_shape, degree=2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    if degree == 1:\n",
    "        # If degree=1, the model is equivalent to a linear model.\n",
    "        expanded = inputs\n",
    "    elif degree == 2:\n",
    "        expanded = PolynomialExpansion(degree=2)(inputs)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only degree=1 and degree=2 are implemented.\")\n",
    "\n",
    "    # The 1x1 convolution learns a linear combination of the expanded polynomial features.\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"linear\")(expanded)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (\n",
    "    sample_batch_features.shape[1],\n",
    "    sample_batch_features.shape[2],\n",
    "    sample_batch_features.shape[3],\n",
    ")\n",
    "poly_model = polynomial_model(input_shape, degree=2)\n",
    "poly_model.summary()\n",
    "\n",
    "\n",
    "poly_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=masked_mse,\n",
    "    metrics=[masked_mae, masked_rmse],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULWEmFVz1yvI"
   },
   "source": [
    "#Testing and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS229Npq1tgb"
   },
   "outputs": [],
   "source": [
    "def reverse_scaling(data, scaler):\n",
    "    return data * scaler.scale_[0] + scaler.mean_[0]\n",
    "\n",
    "\n",
    "def compute_masked_rmse(y_true, y_pred, epsilon=1e-6):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    if y_true_valid.size == 0:\n",
    "        return np.nan\n",
    "    return np.sqrt(np.mean((y_true_valid - y_pred_valid) ** 2))\n",
    "\n",
    "\n",
    "def compute_masked_mae(y_true, y_pred, epsilon=1e-6):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    if y_true_valid.size == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true_valid - y_pred_valid))\n",
    "\n",
    "\n",
    "def compute_masked_bias(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    if y_true_valid.size == 0:\n",
    "        return np.nan\n",
    "    return np.mean(y_pred_valid - y_true_valid)\n",
    "\n",
    "\n",
    "def compute_masked_pearson(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true)\n",
    "    y_true_valid = y_true[mask]\n",
    "    y_pred_valid = y_pred[mask]\n",
    "    if y_true_valid.size == 0:\n",
    "        return np.nan\n",
    "    corr, _ = pearsonr(y_true_valid, y_pred_valid)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dataset, steps, label_scaler):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (x, y) in enumerate(test_dataset.take(steps)):\n",
    "        preds = model.predict(x)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y)\n",
    "\n",
    "    # Concatenate batches along time axis\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Reverse the scaling\n",
    "    all_preds_orig = reverse_scaling(all_preds, label_scaler)\n",
    "    all_labels_orig = reverse_scaling(all_labels, label_scaler)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse = compute_masked_rmse(all_labels_orig, all_preds_orig)\n",
    "    mae = compute_masked_mae(all_labels_orig, all_preds_orig)\n",
    "    bias = compute_masked_bias(all_labels_orig, all_preds_orig)\n",
    "    corr = compute_masked_pearson(all_labels_orig, all_preds_orig)\n",
    "\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Bias (Mean Error): {bias:.4f}\")\n",
    "    print(f\"Pearson Correlation: {corr:.4f}\")\n",
    "\n",
    "    metrics = {\"RMSE\": rmse, \"MAE\": mae, \"Bias\": bias, \"Pearson\": corr}\n",
    "    return all_preds_orig, all_labels_orig, metrics\n",
    "\n",
    "\n",
    "def plot_evaluation_results(all_preds, all_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(all_labels.reshape(-1), all_preds.reshape(-1), alpha=0.2, color=\"blue\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Scatter Plot of True vs. Predicted Values\")\n",
    "    min_val = 0\n",
    "    max_val = 15\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], \"r--\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_table(metrics):\n",
    "    table_data = [\n",
    "        [\"RMSE\", f\"{metrics['RMSE']:.4f}\"],\n",
    "        [\"MAE\", f\"{metrics['MAE']:.4f}\"],\n",
    "        [\"Bias\", f\"{metrics['Bias']:.4f}\"],\n",
    "        [\"Pearson\", f\"{metrics['Pearson']:.4f}\"],\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=[\"Metric\", \"Value\"],\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    plt.title(\"Evaluation Metrics (Masked)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "all_preds_orig, all_labels_orig, metrics = evaluate_model(\n",
    "    model, test_dataset, steps=test_batches, label_scaler=label_scaler\n",
    ")\n",
    "# Plot\n",
    "plot_evaluation_results(all_preds_orig, all_labels_orig)\n",
    "plot_metrics_table(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "VJSz-9U03hi-",
    "outputId": "c3e09233-465b-4022-ad51-aa315aeb3cad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a65ba099c6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# It is assumed that test_dataset yields batches (x, y) with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#    x shape: (batch, height, width, channels) and y shape: (batch, height, width, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Convert tensors to numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch, 76, 260, 6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "all_features_orig = []\n",
    "all_labels_orig = []\n",
    "\n",
    "for x, y in test_dataset:\n",
    "    x_np = x.numpy()\n",
    "    y_np = y.numpy()\n",
    "    x_feature = x_np[..., 0]\n",
    "\n",
    "    x_feature_orig = (\n",
    "        scalers[\"VHM0\"]\n",
    "        .inverse_transform(x_feature.reshape(-1, 1))\n",
    "        .reshape(x_feature.shape)\n",
    "    )\n",
    "    y_label_orig = reverse_scaling(y_np, label_scaler)\n",
    "\n",
    "    all_features_orig.append(x_feature_orig)\n",
    "    all_labels_orig.append(y_label_orig)\n",
    "\n",
    "all_features_orig = np.concatenate(all_features_orig, axis=0)\n",
    "all_labels_orig = np.concatenate(all_labels_orig, axis=0)\n",
    "\n",
    "all_labels_orig = all_labels_orig[..., 0]\n",
    "\n",
    "\n",
    "rmse = compute_masked_rmse(all_labels_orig, all_features_orig)\n",
    "mae = compute_masked_mae(all_labels_orig, all_features_orig)\n",
    "bias = compute_masked_bias(all_labels_orig, all_features_orig)\n",
    "corr = compute_masked_pearson(all_labels_orig, all_features_orig)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"Bias (Mean Error): {bias:.4f}\")\n",
    "print(f\"Pearson Correlation: {corr:.4f}\")\n",
    "metrics = {\"RMSE\": rmse, \"MAE\": mae, \"Bias\": bias, \"Pearson\": corr}\n",
    "\n",
    "\n",
    "def plot_evaluation_results(all_preds, all_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(all_labels.reshape(-1), all_preds.reshape(-1), alpha=0.2, color=\"blue\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Uncorrected Values\")\n",
    "    plt.title(\"Scatter Plot of True vs. Uncorrected Values\")\n",
    "    min_val = 0\n",
    "    max_val = 15\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], \"r--\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_table(metrics):\n",
    "    table_data = [\n",
    "        [\"RMSE\", f\"{metrics['RMSE']:.4f}\"],\n",
    "        [\"MAE\", f\"{metrics['MAE']:.4f}\"],\n",
    "        [\"Bias\", f\"{metrics['Bias']:.4f}\"],\n",
    "        [\"Pearson\", f\"{metrics['Pearson']:.4f}\"],\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=[\"Metric\", \"Value\"],\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    plt.title(\"Metrics of True vs Uncorrected values\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_evaluation_results(all_features_orig, all_labels_orig)\n",
    "plot_metrics_table(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWqFgy2g4tGx"
   },
   "outputs": [],
   "source": [
    "# Metrics for Predictions vs. Labels\n",
    "rmse_map_pred = np.sqrt(np.mean((all_preds_orig - all_labels_orig) ** 2, axis=0))\n",
    "mae_map_pred = np.mean(np.abs(all_preds_orig - all_labels_orig), axis=0)\n",
    "bias_map_pred = np.mean(all_preds_orig - all_labels_orig, axis=0)\n",
    "avg_label_map = np.mean(all_labels_orig, axis=0)\n",
    "# Metrics for Features vs. Labels\n",
    "rmse_map_feat = np.sqrt(np.mean((all_features_orig - all_labels_orig) ** 2, axis=0))\n",
    "mae_map_feat = np.mean(np.abs(all_features_orig - all_labels_orig), axis=0)\n",
    "bias_map_feat = np.mean(all_features_orig - all_labels_orig, axis=0)\n",
    "\n",
    "\n",
    "H, W = avg_label_map.shape\n",
    "land_mask = np.isnan(labels).all(dim=\"time\")\n",
    "\n",
    "# Convert from xarray DataArray to a plain numpy array (if needed):\n",
    "land_mask = land_mask.data  # shape: (lat, lon)\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare a Custom \"jet\" Colormap with Land as White\n",
    "# ---------------------------\n",
    "cmap = plt.get_cmap(\"jet\").copy()\n",
    "cmap.set_bad(\"white\")  # Masked values will appear white\n",
    "\n",
    "# Create masked arrays for all computed maps using the land mask\n",
    "rmse_map_pred_masked = np.ma.array(rmse_map_pred, mask=land_mask)\n",
    "mae_map_pred_masked = np.ma.array(mae_map_pred, mask=land_mask)\n",
    "bias_map_pred_masked = np.ma.array(bias_map_pred, mask=land_mask)\n",
    "\n",
    "rmse_map_feat_masked = np.ma.array(rmse_map_feat, mask=land_mask)\n",
    "mae_map_feat_masked = np.ma.array(mae_map_feat, mask=land_mask)\n",
    "bias_map_feat_masked = np.ma.array(bias_map_feat, mask=land_mask)\n",
    "\n",
    "\n",
    "# Difference Maps\n",
    "diff_rmse_map = rmse_map_feat - rmse_map_pred\n",
    "diff_mae_map = mae_map_feat - mae_map_pred\n",
    "diff_bias_map = bias_map_feat - bias_map_pred\n",
    "\n",
    "diff_rmse_map_masked = np.ma.array(diff_rmse_map, mask=land_mask)\n",
    "diff_mae_map_masked = np.ma.array(diff_mae_map, mask=land_mask)\n",
    "diff_bias_map_masked = np.ma.array(diff_bias_map, mask=land_mask)\n",
    "# Compute global min/max for each metric separately:\n",
    "rmse_vmin = 0\n",
    "rmse_vmax = 0.35\n",
    "\n",
    "mae_vmin = 0\n",
    "mae_vmax = 0.2\n",
    "\n",
    "bias_vmin = -0.15\n",
    "bias_vmax = 0.15\n",
    "\n",
    "\n",
    "def plot_map(ax, data, title, mask, vmin, vmax):\n",
    "    im = ax.imshow(data, cmap=cmap, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.contour(\n",
    "        mask.astype(float), levels=[0.5], colors=\"black\", linewidths=1, origin=\"lower\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Predictions vs. Labels\n",
    "fig1, axes1 = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plot_map(\n",
    "    axes1[0],\n",
    "    rmse_map_pred_masked,\n",
    "    \"RMSE Map (Predictions vs. Labels)\",\n",
    "    land_mask,\n",
    "    rmse_vmin,\n",
    "    rmse_vmax,\n",
    ")\n",
    "plot_map(\n",
    "    axes1[1],\n",
    "    mae_map_pred_masked,\n",
    "    \"MAE Map (Predictions vs. Labels)\",\n",
    "    land_mask,\n",
    "    mae_vmin,\n",
    "    mae_vmax,\n",
    ")\n",
    "plot_map(\n",
    "    axes1[2],\n",
    "    bias_map_pred_masked,\n",
    "    \"Bias Map (Predictions vs. Labels)\",\n",
    "    land_mask,\n",
    "    bias_vmin,\n",
    "    bias_vmax,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Features vs. Labels\n",
    "fig2, axes2 = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plot_map(\n",
    "    axes2[0],\n",
    "    rmse_map_feat_masked,\n",
    "    \"RMSE Map (Features vs. Labels)\",\n",
    "    land_mask,\n",
    "    rmse_vmin,\n",
    "    rmse_vmax,\n",
    ")\n",
    "plot_map(\n",
    "    axes2[1],\n",
    "    mae_map_feat_masked,\n",
    "    \"MAE Map (Features vs. Labels)\",\n",
    "    land_mask,\n",
    "    mae_vmin,\n",
    "    mae_vmax,\n",
    ")\n",
    "plot_map(\n",
    "    axes2[2],\n",
    "    bias_map_feat_masked,\n",
    "    \"Bias Map (Features vs. Labels)\",\n",
    "    land_mask,\n",
    "    bias_vmin,\n",
    "    bias_vmax,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Difference Maps\n",
    "fig3, axes3 = plt.subplots(1, 3, figsize=(20, 6))\n",
    "plot_map(\n",
    "    axes3[0],\n",
    "    diff_rmse_map_masked,\n",
    "    \"Difference RMSE Map (Features - Predictions)\",\n",
    "    land_mask,\n",
    "    -0.05,\n",
    "    0.04,\n",
    ")\n",
    "plot_map(\n",
    "    axes3[1],\n",
    "    diff_mae_map_masked,\n",
    "    \"Difference MAE Map (Features - Predictions)\",\n",
    "    land_mask,\n",
    "    -0.05,\n",
    "    0.04,\n",
    ")\n",
    "plot_map(\n",
    "    axes3[2],\n",
    "    diff_bias_map_masked,\n",
    "    \"Difference Bias Map (Features - Predictions)\",\n",
    "    land_mask,\n",
    "    -0.09,\n",
    "    0,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "AOQJfkaQ5jSS",
    "outputId": "0ad92021-dae2-4a15-f4ae-b2859386335b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_preds_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7cfd6fe4cb93>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Compute metrics for predictions vs. labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     r_pred, m_pred, b_pred, count = compute_metrics_for_bin(all_labels_orig, all_preds_orig,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                                             bin_edges[i], bin_edges[i+1])\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Compute metrics for features vs. labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_preds_orig' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_metrics_for_bin(y_true, y_pred, bin_lower, bin_upper):\n",
    "    mask = (y_true >= bin_lower) & (y_true < bin_upper) & (~np.isnan(y_true))\n",
    "    y_true_bin = y_true[mask]\n",
    "    y_pred_bin = y_pred[mask]\n",
    "    sample_count = y_true_bin.size\n",
    "    if sample_count == 0:\n",
    "        return np.nan, np.nan, np.nan, 0\n",
    "    rmse = np.sqrt(np.mean((y_true_bin - y_pred_bin) ** 2))\n",
    "    mae = np.mean(np.abs(y_true_bin - y_pred_bin))\n",
    "    bias = np.mean(y_pred_bin - y_true_bin)\n",
    "    return rmse, mae, bias, sample_count\n",
    "\n",
    "\n",
    "bin_edges = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "bin_labels = [f\"{bin_edges[i]}-{bin_edges[i + 1]}\" for i in range(len(bin_edges) - 1)]\n",
    "n_bins = len(bin_edges) - 1\n",
    "\n",
    "\n",
    "rmse_pred, mae_pred, bias_pred = [], [], []\n",
    "rmse_feat, mae_feat, bias_feat = [], [], []\n",
    "sample_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    r_pred, m_pred, b_pred, count = compute_metrics_for_bin(\n",
    "        all_labels_orig, all_preds_orig, bin_edges[i], bin_edges[i + 1]\n",
    "    )\n",
    "    r_feat, m_feat, b_feat, count_feat = compute_metrics_for_bin(\n",
    "        all_labels_orig, all_features_orig, bin_edges[i], bin_edges[i + 1]\n",
    "    )\n",
    "    rmse_pred.append(r_pred)\n",
    "    mae_pred.append(m_pred)\n",
    "    bias_pred.append(b_pred)\n",
    "    rmse_feat.append(r_feat)\n",
    "    mae_feat.append(m_feat)\n",
    "    bias_feat.append(b_feat)\n",
    "    sample_counts.append(count)\n",
    "\n",
    "bar_width = 0.35\n",
    "x = np.arange(n_bins)\n",
    "\n",
    "# RMSE Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars1 = plt.bar(\n",
    "    x - bar_width / 2,\n",
    "    rmse_feat,\n",
    "    width=bar_width,\n",
    "    label=\"Features vs. Labels\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "bars2 = plt.bar(\n",
    "    x + bar_width / 2,\n",
    "    rmse_pred,\n",
    "    width=bar_width,\n",
    "    label=\"Predictions vs. Labels\",\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "plt.xticks(x, bin_labels)\n",
    "plt.xlabel(\"VHM0 Range (m)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE for Each VHM0 Range\")\n",
    "plt.ylim(0, 1.1)\n",
    "# sample size above each bin group\n",
    "for i in range(n_bins):\n",
    "    y_val = max(rmse_feat[i], rmse_pred[i])\n",
    "    plt.text(\n",
    "        x[i],\n",
    "        y_val + 0.05 * y_val,\n",
    "        f\"n={sample_counts[i]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# MAE Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars1 = plt.bar(\n",
    "    x - bar_width / 2,\n",
    "    mae_feat,\n",
    "    width=bar_width,\n",
    "    label=\"Features vs. Labels\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "bars2 = plt.bar(\n",
    "    x + bar_width / 2,\n",
    "    mae_pred,\n",
    "    width=bar_width,\n",
    "    label=\"Predictions vs. Labels\",\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "plt.xticks(x, bin_labels)\n",
    "plt.xlabel(\"VHM0 Range (m)\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"MAE for Each VHM0 Range\")\n",
    "plt.ylim(0, 1.1)\n",
    "for i in range(n_bins):\n",
    "    y_val = max(mae_feat[i], mae_pred[i])\n",
    "    plt.text(\n",
    "        x[i],\n",
    "        y_val + 0.05 * y_val,\n",
    "        f\"n={sample_counts[i]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Bias Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars1 = plt.bar(\n",
    "    x - bar_width / 2,\n",
    "    bias_feat,\n",
    "    width=bar_width,\n",
    "    label=\"Features vs. Labels\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "bars2 = plt.bar(\n",
    "    x + bar_width / 2,\n",
    "    bias_pred,\n",
    "    width=bar_width,\n",
    "    label=\"Predictions vs. Labels\",\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "plt.xticks(x, bin_labels)\n",
    "plt.xlabel(\"VHM0 Range (m)\")\n",
    "plt.ylabel(\"Bias\")\n",
    "plt.title(\"Bias for Each VHM0 Range\")\n",
    "plt.ylim(-1.1, 0.2)\n",
    "for i in range(n_bins):\n",
    "    y_val = max(bias_feat[i], bias_pred[i])\n",
    "    # Adjust offset differently if bias is negative.\n",
    "    offset = 0.05 * abs(y_val) if y_val != 0 else 0.05\n",
    "    plt.text(\n",
    "        x[i],\n",
    "        y_val + offset,\n",
    "        f\"n={sample_counts[i]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5cono_zvI1iz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}